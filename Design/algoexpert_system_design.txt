AlgoExpert System Design Fundamentals


3. Client Server Model: 

        Lets look at an example
        
        Client: browser 
            - DNS: 
                - IP

        
        Server: AlgoExpert backend

        Client: A machine or process that requests data or service from a server

        Node: a single machine can be both the client and a server at the same times

        IP address: an address given to each machine connected the public internet. IPV4 address consists of four numbers seperated by dots.  where all 4 values are from 0 - 255

        In order for multiple programs to listen for a new network connection without colliding, they pick a port:
            here are some predefined example ports:
                22: Secure Shell
                53: DNS lookup
                80: http
                443: HTTPS 


4. Network protocols: 
    IP: Internet Protocol address
        - IP packet: data sent from one machine to another
        - IP address: a unique address given to each machine connected to the public internet
        - IP packets: are made up of bytes

    TCP: Transmission Control Protocol
        - built on top of the IP
        - send IP packets in an ordered way
        - allows you to send arbitrarily large pieces of data

    HTTP: 
        - Hypertext Transfer Protocol
        - Built on top of TCP
        - introduces higher level of abstraction on top of TCP and IP
        - basically a way of communicating in a request and response paradigm
        - HTTP is a request/response protocol that is much more applicable in sys design questions.

5. Storage

        - Database 
            - write data
            - read dat
            - just a server
            
            - You can write data to: 
                - disk: to a physical hard drive
                - memory: data is stored in memory pros because it is faster to access but if it goes down, it will be lost


6. Latency and Throughput 
    - 2 most important measure of the performance of a System
        - Latency: how long it takes to send a packet
        - Throughput: how much data can be sent in a given amount of time

    Latency:
        - if you are reading 1MB sequentially of data in 1 second from memory, that takes 250 microseconds
        - if you are reading 1MB sequentially of data in 1 second from SSD, that takes 1000 microseconds
        - if you are sending 1MB sequentially of data in 1GB/second network, that takes 10k microseconds
        - if you are sending 1MB sequentially of data in HDD, that takes 20k microseconds
        - if you are sending a packet (smaller than 1MB ) sequentially from Cali to Neth, to Cali, that takes 150k microseconds

    Throughput:
        - if you are reading 1MB sequentially of data in 1 second from memory, that takes 250 microseconds-
    
    - examples:
        - video games: those systems can have fast connection or slow(lag) because of how far a server is
    - in  sys design, we need to understand the tradeoff


    - How to handle throughput: 
        (naive)
        - you can pay your server provider to "increase" throughput
        
        (better)
        - have multiple servers handling the request

    - you can't make assumpstions on latency & throughput based on the other, they are not necessary correlated 

    
7. Availability 
    - when dealing with availability, we are dealing with high percentages
    - in the industry, we use 9's to use availability. so like if we had 99.99% availability, we would say 4 9's 
    - 5 9's is the gold standard of availability

    Service providers have a SLA 
    - SLA: Service Level Agreement
    - customers agree to their SLA when using their service, as you can see in like namecheap they guarentee 99.99% availability
    - SLO: Service Level Objective which is components of SLA.

    During system design interviews:
        - what part of the system requires high availability and what can use less availability

    How do we improve the availability of our system:
        - system doesn't have single points of failure (solve with redundancy)
        - have load-balancers to distribute traffic
        

8. Caching
    - Caching is used to reduce redundancy in our system.
    - caching is helpful when you have computational long  running tasks that take a long time to complete
    - you are able to client side or server side cache the data in order to speed up access to data
    - if you don't need frequent access to data, you can cache it.


    - you can actually call your db to update cache to server side cache then you won't have to hit db when client hits server

    - Writeback cache:
        - Whenever a  use makes a request the cache is updated, but then later asynchronously, the cache is written back to the database.
        - Con: if you lose data in cache then database might not get updated

    - Caches can become stale if not updated properly
        - A solution could be move your cache out of server, then server hits memory like redis then you have a source of truth for cache 

    - Popular policy 
        - LRU Cache: least recently used is likely to be least cared about
        - Least frequently used is likely to get rid of first

9. Proxies
    - Forward Proxy is on a client side:
        - Client sends request to server, instead of going directly to server it goes to forward proxy which then requests forward to server
        - server will give response to proxy, then proxy will give response to client
    - Reverse Proxy (basically opposite of forward proxy) Client thinks it is interacting with the server.
        - Server sends request to client, instead of going directly to client it goes to reverse proxy which then requests to client
        - client will give response to proxy, then proxy will give response to server

    VPN's are basically forward proxies

    - Reverse Proxies are really useful: 
        - you can filter our request you want to ignore
        - you can cache certain things on reverse proxy
        - you can use it as a load balancer, which is a server which effectively distributes traffic to multiple servers.


10. Load Balancing
    - our single server is limited
    - the more requests sent to server, we can become overloaded. 
    - load balancing is a technique to distribute traffic to multiple servers
    - Vertical Scale: Increase power of server
    - Horizontal Scale: Increase number of servers

    - A load balancer is a server between cliets and servers
    - This increases throughput and latency
    - How does a load balancer know to distribute traffic:
        - people configuring the load balancers: 

            1. pure random redirect
            2. round robin - goes through all your servers top to bottom, the again just like a queue of servers again and again
            3. Weighted round robin - might put more weight on certain servers
            4. IP based which could cache request to specific server
    - in interviews you want to select different server selection strategies


11. Hashing
    - can store client request via:
        - Modulus hashing  client # % amount of servers = server # this client will belong to
            - Cons: what abut is a server dies or need to add more servers. well this will break. 
        - How to solve:  
        - Consistent Hashing:
            - 
        - Rendezvous Hashing: 

